
@article{kervadec_boundary_2021,
	title = {Boundary loss for highly unbalanced segmentation},
	volume = {67},
	issn = {13618415},
	url = {http://arxiv.org/abs/1812.07032},
	doi = {10.1016/j.media.2020.101851},
	abstract = {Widely used loss functions for {CNN} segmentation, e.g., Dice or cross-entropy, are based on integrals over the segmentation regions. Unfortunately, for highly unbalanced segmentations, such regional summations have values that differ by several orders of magnitude across classes, which affects training performance and stability. We propose a boundary loss, which takes the form of a distance metric on the space of contours, not regions. This can mitigate the difficulties of highly unbalanced problems because it uses integrals over the interface between regions instead of unbalanced integrals over the regions. Furthermore, a boundary loss complements regional information. Inspired by graph-based optimization techniques for computing active-contour flows, we express a non-symmetric \$L\_2\$ distance on the space of contours as a regional integral, which avoids completely local differential computations involving contour points. This yields a boundary loss expressed with the regional softmax probability outputs of the network, which can be easily combined with standard regional losses and implemented with any existing deep network architecture for N-D segmentation. We report comprehensive evaluations and comparisons on different unbalanced problems, showing that our boundary loss can yield significant increases in performances while improving training stability. Our code is publicly available: https://github.com/{LIVIAETS}/surface-loss .},
	pages = {101851},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Kervadec, Hoel and Bouchtiba, Jihene and Desrosiers, Christian and Granger, Eric and Dolz, Jose and Ayed, Ismail Ben},
	urldate = {2023-11-03},
	date = {2021-01},
	eprinttype = {arxiv},
	eprint = {1812.07032 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{ma_how_2020,
	title = {How Distance Transform Maps Boost Segmentation {CNNs}: An Empirical Study},
	url = {https://proceedings.mlr.press/v121/ma20b.html},
	shorttitle = {How Distance Transform Maps Boost Segmentation {CNNs}},
	abstract = {Incorporating distance transform maps of ground truth into segmentation {CNNs} has been an interesting new trend in the last data. Despite many great works leading to improvements on a variety of segmentation tasks, the comparison among these methods has not been well studied. In this paper, our \{{\textbackslash}em first contribution\} is to summarize the latest developments of these methods in the 3D medical segmentation field. The \{{\textbackslash}em second contribution\} is that we systematically evaluated five benchmark methods on two representative public datasets. These experiments highlight that all the five benchmark methods can bring performance gains to baseline V-Net. However, the implementation details have a noticeable impact on the performance, and not all the methods hold the benefits on different datasets. Finally, we suggest the best practices and indicate unsolved problems for incorporating distance transform maps into {CNNs}, which we hope would be useful for the community. The codes and trained models are publicly available at: \{https://github.com/{JunMa}11/{SegWithDistMap}\}.},
	eventtitle = {Medical Imaging with Deep Learning},
	pages = {479--492},
	booktitle = {Proceedings of the Third Conference on Medical Imaging with Deep Learning},
	publisher = {{PMLR}},
	author = {Ma, Jun and Wei, Zhan and Zhang, Yiwen and Wang, Yixin and Lv, Rongfei and Zhu, Cheng and Gaoxiang, Chen and Liu, Jianan and Peng, Chao and Wang, Lei and Wang, Yunpeng and Chen, Jianan},
	urldate = {2023-11-25},
	date = {2020-09-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{ribalta_lorenzo_multi-modal_2020,
	location = {Cham},
	title = {Multi-modal U-Nets with Boundary Loss and Pre-training for Brain Tumor Segmentation},
	isbn = {978-3-030-46643-5},
	doi = {10.1007/978-3-030-46643-5_13},
	series = {Lecture Notes in Computer Science},
	abstract = {Gliomas are the most common primary brain tumors, and their manual segmentation is a time-consuming and user-dependent process. We present a two-step multi-modal U-Net-based architecture with unsupervised pre-training and surface loss component for brain tumor segmentation which allows us to seamlessly benefit from all magnetic resonance modalities during the delineation. The results of the experimental study, performed over the newest release of the {BraTS} test set, revealed that our method delivers accurate brain tumor segmentation, with the average {DICE} score of 0.72, 0.86, and 0.77 for the enhancing tumor, whole tumor, and tumor core, respectively. The total time required to process one study using our approach amounts to around 20Â s.},
	pages = {135--147},
	booktitle = {Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries},
	publisher = {Springer International Publishing},
	author = {Ribalta Lorenzo, Pablo and Marcinkiewicz, Michal and Nalepa, Jakub},
	editor = {Crimi, Alessandro and Bakas, Spyridon},
	date = {2020},
	langid = {english},
	keywords = {Boundary loss, Brain tumor, Segmentation, U-Net},
}
@misc{karimi_reducing_2019,
	title = {Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1904.10030},
	abstract = {The Hausdorff Distance ({HD}) is widely used in evaluating medical image segmentation methods. However, existing segmentation methods do not attempt to reduce {HD} directly. In this paper, we present novel loss functions for training convolutional neural network ({CNN})-based segmentation methods with the goal of reducing {HD} directly. We propose three methods to estimate {HD} from the segmentation probability map produced by a {CNN}. One method makes use of the distance transform of the segmentation boundary. Another method is based on applying morphological erosion on the difference between the true and estimated segmentation maps. The third method works by applying circular/spherical convolution kernels of different radii on the segmentation probability maps. Based on these three methods for estimating {HD}, we suggest three loss functions that can be used for training to reduce {HD}. We use these loss functions to train {CNNs} for segmentation of the prostate, liver, and pancreas in ultrasound, magnetic resonance, and computed tomography images and compare the results with commonly-used loss functions. Our results show that the proposed loss functions can lead to approximately 18-45 \% reduction in {HD} without degrading other segmentation performance criteria such as the Dice similarity coefficient. The proposed loss functions can be used for training medical image segmentation methods in order to reduce the large segmentation errors.},
	number = {{arXiv}:1904.10030},
	publisher = {{arXiv}},
	author = {Karimi, Davood and Salcudean, Septimiu E.},
	urldate = {2023-11-26},
	date = {2019-04-22},
	eprinttype = {arxiv},
	eprint = {1904.10030 [cs, eess, stat]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}


@misc{xue_shape-aware_2019,
	title = {Shape-Aware Organ Segmentation by Predicting Signed Distance Maps},
	url = {http://arxiv.org/abs/1912.03849},
	abstract = {In this work, we propose to resolve the issue existing in current deep learning based organ segmentation systems that they often produce results that do not capture the overall shape of the target organ and often lack smoothness. Since there is a rigorous mapping between the Signed Distance Map ({SDM}) calculated from object boundary contours and the binary segmentation map, we exploit the feasibility of learning the {SDM} directly from medical scans. By converting the segmentation task into predicting an {SDM}, we show that our proposed method retains superior segmentation performance and has better smoothness and continuity in shape. To leverage the complementary information in traditional segmentation training, we introduce an approximated Heaviside function to train the model by predicting {SDMs} and segmentation maps simultaneously. We validate our proposed models by conducting extensive experiments on a hippocampus segmentation dataset and the public {MICCAI} 2015 Head and Neck Auto Segmentation Challenge dataset with multiple organs. While our carefully designed backbone 3D segmentation network improves the Dice coefficient by more than 5\% compared to current state-of-the-arts, the proposed model with {SDM} learning produces smoother segmentation results with smaller Hausdorff distance and average surface distance, thus proving the effectiveness of our method.},
	number = {{arXiv}:1912.03849},
	publisher = {{arXiv}},
	author = {Xue, Yuan and Tang, Hui and Qiao, Zhi and Gong, Guanzhong and Yin, Yong and Qian, Zhen and Huang, Chao and Fan, Wei and Huang, Xiaolei},
	urldate = {2023-11-26},
	date = {2019-12-08},
	eprinttype = {arxiv},
	eprint = {1912.03849 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\INES\\Zotero\\storage\\XWVUCMEG\\1912.html:text/html;Full Text PDF:C\:\\Users\\INES\\Zotero\\storage\\LT5J2EPB\\Xue et al. - 2019 - Shape-Aware Organ Segmentation by Predicting Signe.pdf:application/pdf},
}

@incollection{sudre_generalised_2017,
	title = {Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations},
	volume = {10553},
	url = {http://arxiv.org/abs/1707.03237},
	abstract = {Deep-learning has proved in recent datas to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
	pages = {240--248},
	author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, SÃ©bastien and Cardoso, M. Jorge},
	urldate = {2023-11-22},
	date = {2017},
	doi = {10.1007/978-3-319-67558-9_28},
	eprinttype = {arxiv},
	eprint = {1707.03237 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},}

@misc{lin_focal_2018,
	title = {Focal Loss for Dense Object Detection},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-{CNN}, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call {RetinaNet}. Our results show that when trained with the focal loss, {RetinaNet} is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	number = {{arXiv}:1708.02002},
	publisher = {{arXiv}},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and DollÃ¡r, Piotr},
	urldate = {2023-11-26},
	date = {2018-02-07},
	eprinttype = {arxiv},
	eprint = {1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@article{bernard2018deep,
  title={Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?},
  author={Bernard, Olivier and Lalande, Alain and Zotti, Clement and Cervenansky, Frederick and Yang, Xin and Heng, Pheng-Ann and Cetin, Irem and Lekadir, Karim and Camara, Oscar and Ballester, Miguel Angel Gonzalez and others},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={11},
  pages={2514--2525},
  data={2018},
  publisher={ieee}
}

@article{staal2004ridge,
  title={Ridge-based vessel segmentation in color images of the retina},
  author={Staal, Joes and Abr{\`a}moff, Michael D and Niemeijer, Meindert and Viergever, Max A and Van Ginneken, Bram},
  journal={IEEE transactions on medical imaging},
  volume={23},
  number={4},
  pages={501--509},
  data={2004},
  publisher={IEEE}
}


@article{paszke2016enet,
  title={Enet: A deep neural network architecture for real-time semantic segmentation},
  author={Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal={arXiv preprint arXiv:1606.02147},
  data={2016}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  data={2014}
}
